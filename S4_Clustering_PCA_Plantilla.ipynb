{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1EQ5G9rQcwE"
      },
      "source": [
        "# Clustering and PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Toc_OzXYQcwH"
      },
      "source": [
        "### Mushroom Dataset\n",
        "\n",
        "Podeis obtener el conjunto de datos en el siguiente enlace:\n",
        "\n",
        "[Mushroom Dataset](https://www.kaggle.com/uciml/mushroom-classification)\n",
        "\n",
        "Como podréis comprobar, hay muchas variables, todas ellas categóricas, por lo que exploraciones con scatterplot no nos serán útiles como en otros casos.\n",
        "\n",
        "La variable a predecir ``class`` es binaria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJc845c11KbK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "RWIOS2y_QcwH"
      },
      "outputs": [],
      "source": [
        "# Carga de librerías, las que hemos considerado básicas, añadid lo que queráis :)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ruta para guardar las tablas y reportes\n",
        "ruta='C:/4_F5/012_noSupervisado/Nat_noSupervisado/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Carga los datos del archivo CSV.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Ruta al archivo CSV.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame con los datos cargados o None si hay un error.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener la ruta absoluta del archivo para evitar problemas con rutas relativas\n",
        "    abs_file_path = os.path.abspath(file_path)\n",
        "\n",
        "    # Verificar si el archivo existe en la ruta especificada\n",
        "    if not os.path.exists(abs_file_path):\n",
        "        # Imprimir mensajes de error detallados si el archivo no existe\n",
        "        print(f\"Error: El archivo no existe en la ruta: {abs_file_path}\")\n",
        "        print(f\"Directorio actual: {os.getcwd()}\")  # Mostrar el directorio actual para ayudar a depurar\n",
        "        print(\"Contenido del directorio:\")\n",
        "        try:\n",
        "            # Intentar listar el contenido del directorio para ayudar a identificar el problema\n",
        "            print(os.listdir(os.path.dirname(abs_file_path)))\n",
        "        except FileNotFoundError:\n",
        "            print(\"El directorio no existe.\")  # Informar si el directorio tampoco existe\n",
        "        return None  # Devolver None para indicar que la carga falló\n",
        "\n",
        "    try:\n",
        "        # Intentar cargar los datos utilizando pandas\n",
        "        df = pd.read_csv(abs_file_path)\n",
        "        print(f\"Datos cargados exitosamente. Shape: {df.shape}\")  # Confirmar la carga exitosa\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        # Capturar cualquier excepción que ocurra durante la carga e imprimir un mensaje de error\n",
        "        print(f\"Error al cargar los datos: {e}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bloque de ejemplo de uso (solo se ejecuta si el script se ejecuta directamente)\n",
        "if __name__ == \"__main__\":\n",
        "    # Obtener la ruta al archivo CSV dentro de la estructura del proyecto\n",
        "    #script_dir = os.path.dirname(os.path.abspath(__file__)) #sólo sirve cuando es .py porque la variable __file__ no está disponible porque no se está ejecutando un script de Python directamente.\n",
        "    #project_root = os.path.dirname(os.path.dirname(script_dir))\n",
        "    #data_path = os.path.join(project_root, \"data\", \"mushrooms.csv\") #(project_root,\"carpeta\",\"subcarpeta\",\"archivo.csv\")\n",
        "    data_path = data_path = os.path.join(os.getcwd(), \"data\", \"mushrooms.csv\")\n",
        "\n",
        "    print(f\"Intentando cargar el archivo desde: {data_path}\")\n",
        "\n",
        "\n",
        "    # Llamar a la función load_data para cargar los datos\n",
        "    \n",
        "    df = load_data(data_path)\n",
        "    if df is not None:\n",
        "        # Imprimir las primeras filas del DataFrame si la carga fue exitosa\n",
        "        print(df.head())\n",
        "    else:\n",
        "        # Imprimir un mensaje de error si la carga falló\n",
        "        print(\"No se pudieron cargar los datos. Verifica la ruta del archivo y su existencia.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KbJPDrpQcwI"
      },
      "source": [
        "### Leer conjunto de datos y primer vistazo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFJoAIsVQcwI",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Leer el csv y sacar por pantalla las cinco primeras filas.\n",
        "\n",
        "def lectura_csv(df):\n",
        "    return df.head()\n",
        "print(lectura_csv(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La librería OS nos permite navegar entre los directorios del sistema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dePM9qXKQcwJ"
      },
      "source": [
        "### Exploración de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45TsUuwkQcwJ"
      },
      "outputs": [],
      "source": [
        "# Descripción del conjunto de datos, estándard.\n",
        "\n",
        "\n",
        "print(df.describe(include='object').T) # con el parámetro include='object' para obtener estadísticas descriptivas para las columnas categóricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJG-fxzJQcwJ"
      },
      "outputs": [],
      "source": [
        "# Información sobre el tipo de datos de cada feature.\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xXz4mT0QcwJ",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "print(f'El número de filas del dataset es:\\n{df.shape[0]:,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Igual que otras veces, una linea, contar los nulos por variable.\n",
        "def count_missing(df):\n",
        "    missing_values = df.isnull().sum()\n",
        "    if (missing_values == 0).all():\n",
        "        return \"El conteo por columna de valores 'missing' resulta 0. Es decir, no hay valores 'missing'\"\n",
        "    else:\n",
        "        return missing_values\n",
        "\n",
        "print(count_missing(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---- Verifica si hay columnas duplicadas\n",
        "duplicated_cols = df.columns[df.columns.duplicated()]\n",
        "\n",
        "if len(duplicated_cols) > 0:\n",
        "    print(\"Columnas duplicadas:\", duplicated_cols)\n",
        "else:\n",
        "    print(\"No hay columnas duplicadas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "    print(df[col].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# Crear una figura con múltiples subplots\n",
        "fig, axs = plt.subplots(nrows=23, ncols=1, figsize=(8, 60))\n",
        "\n",
        "# Iterar sobre cada variable y crear un gráfico de barras\n",
        "for i, variable in enumerate(df.columns):\n",
        "    sns.countplot(x=variable, data=df, ax=axs[i])\n",
        "    axs[i].set_title(f'Distribución de {variable}')\n",
        "\n",
        "# Ajustar el layout para que los títulos no se superpongan\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Considero que valores 'outliers' son aquellos que suponen un 1% del total de registros\n",
        "for col in df.columns:\n",
        "    print(df[col].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJv-ez3WQcwK"
      },
      "source": [
        "#### Buscar valores extraños. Para ello, ver los valores únicos en cada feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUZ2EHmTQcwK"
      },
      "outputs": [],
      "source": [
        "# Obtener un nuevo dataframe de dos columnas donde en la primera estén las features (features) y en la otra los valores únicos\n",
        "# asociados (n_values).\n",
        "\n",
        "values_unique=pd.DataFrame({'categories':df.nunique()})\n",
        "print(values_unique)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXIyz_tdQcwK"
      },
      "source": [
        "#### Tratar aquellos valores que entendamos que sean nulos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "OVQnxK1gQcwK"
      },
      "outputs": [],
      "source": [
        "# Imputaciones. Podéis quitar esos puntos (fila entera), imputar con la moda o dejar ese valor como una posibilidad más."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dbmx1Z7QcwK"
      },
      "source": [
        "#### Mirad cuántos valores hay en cada feature, ¿Todas las features aportan información? Si alguna no aporta información, eliminadla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts2xeUavQcwK"
      },
      "outputs": [],
      "source": [
        "# Dejar por el camino si procede.\n",
        "#Diluyo el df para que se expanda cada features según su categoría\n",
        "melted_df = pd.melt(df, var_name='Features', value_name='Categories')\n",
        "grouped_df = melted_df.groupby(['Features', 'Categories']).size().reset_index(name='count_values')\n",
        "\n",
        "category_totals = grouped_df.groupby('Features')['count_values'].transform('sum')\n",
        "#machaco la variable\n",
        "grouped_df['ratio']=round((grouped_df['count_values'] / category_totals ),2)\n",
        "print(grouped_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "grouped_df.to_csv(ruta+'tables/df_grouped.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Voy a filtrar el dataframe agrupado para que --> ¿Qué hago con los ?\n",
        "#count = (grouped_df['ratio'] < 0.01).sum()\n",
        "under_one = grouped_df[grouped_df['ratio'] < 0.01]\n",
        "print(f\"Valores menores a 0.01 en la columna 'ratio': {(grouped_df['ratio'] < 0.01).sum()}\")\n",
        "print(under_one)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dX1LM1VQcwK"
      },
      "source": [
        "#### Separar entre variables predictoras y variables a predecir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS9HEA2eQcwK"
      },
      "outputs": [],
      "source": [
        "# La variable que trata de predecir este conjunto de datos es 'class'.\n",
        "# Por tanto la variable objetivo es 'y'\n",
        "# las variables predictoras son las columnas son 'x'\n",
        "y = df['class']\n",
        "print(y.head())\n",
        "x =df.drop(columns='class')\n",
        "\n",
        "print(x.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "########## Hago ingeniería de características?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'x'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[168], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##### Miro la correlación entre las variables predictoras\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m corr_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(corr_matrix)\n",
            "File \u001b[1;32mc:\\Users\\nel_n\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:11049\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  11047\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m  11048\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m> 11049\u001b[0m mat \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m  11051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  11052\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
            "File \u001b[1;32mc:\\Users\\nel_n\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:1993\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1992\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m-> 1993\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[0;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(result, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
            "File \u001b[1;32mc:\\Users\\nel_n\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1688\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(blk\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[0;32m   1691\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview()\n",
            "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'x'"
          ]
        }
      ],
      "source": [
        "##### Miro la correlación entre las variables predictoras para ello hay que codificar primero\n",
        "corr_matrix = x.corr()\n",
        "print(corr_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN1fZZfZQcwL"
      },
      "source": [
        "#### Codificar correctamente las variables categóricas a numéricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4l92dfEQcwL"
      },
      "outputs": [],
      "source": [
        "# One Hot Encoder (una linea).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBBAM0bzQcwL"
      },
      "source": [
        "#### Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHxqeJQqQcwL"
      },
      "outputs": [],
      "source": [
        "# Os lo dejamos a todos igual\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sL57bzCQcwL"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhAhJqbKQcwM"
      },
      "source": [
        "Es un conjunto de datos del que aún no hemos visto nada (no tenemos graficas) así que vamos a hacer algunas. Tenemos el problema de que son muchas variables, **PCA al rescate**: le pedimos que nos de dos dimensiones y las pintamos, sabemos que serán **aquellas que retengan más información**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3232KSn9QcwM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "pca =       # metodo de sklearn\n",
        "pca.fit(X_train)\n",
        "\n",
        "# Representar en un scatterplot y poner en color las etiquetas de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMYH_Hv0QcwM"
      },
      "source": [
        "Parece que está bastante separadito, parece que a ojo mucho se puede ver :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdE0AvlKQcwM"
      },
      "source": [
        "Igualmente, vamos a entrenar un clasificador a ver qué tal lo hace antes de editar más"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKQqz_EPQcwM"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1. Definir el clasificador y el número de estimadores\n",
        "# 2. Entrenar en train\n",
        "# 3. Calcular la precisión sobre test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PACQlU5_QcwM"
      },
      "source": [
        "Es un conjunto sencillo y Random Forest es muy bueno en su trabajo, Igualmente, vamos a ver qué tamaño tenemos de dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODibK0D2QcwN"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rEVhvRaQcwN"
      },
      "source": [
        "¿Muchas features no? Vamos a reducir las usando PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEJPZw_cQcwN",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "n_features = # definir un rango de valores a probar\n",
        "scores = []\n",
        "\n",
        "for n in n_features:\n",
        "\n",
        "    # Hacer PCA sobre X_train\n",
        "    # 1. Definir PCA\n",
        "    # 2. Aprender PCA sobre X_train\n",
        "\n",
        "    # Entrenar Random Forest\n",
        "    # 1. Definir el RF\n",
        "    # 2. Entrenar clasificador\n",
        "\n",
        "    # Guardar el score\n",
        "\n",
        "\n",
        "sns.lineplot(x=n_features, y=scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKjl6aHiQcwN"
      },
      "source": [
        "Vale, estamos viendo que a partir de unas 10 features ya tenemos el score que queríamos y además hemos reducido las variables a un 10% de las que teníamos, incluso menos que las variables originales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jvqa-leQcwN"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWiMHKUdQcwN"
      },
      "source": [
        "Viendo que el conjunto de datos es sencillito, podemos intentar hacer algo de clustering a ver qué información podemos obtener.\n",
        "\n",
        "El primer paso va a ser importar la función de Kmeans de sklearn, y a partir de ahi, vamos a buscar el valor óptimo de clusters. Como hemos visto anteriormente, este valor lo obtenemos, por ejemplo, del codo de la gráfica que representa el total de las distancias de los puntos a los centros de los clusters asociados. Os dejo la página de la documentación de sklearn para que lo busquéis:\n",
        "\n",
        "[K-Means on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
        "\n",
        "Con esto solo hay que ahora generar los modelos de kmeans, evaluar y pintar la gráfica para los valores de ``k`` que establezcais.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV0IXFncQcwO"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "scores = []\n",
        "k_values = # definir un rango\n",
        "for a in k_values:\n",
        "\n",
        "    # Definir Kmeans y ajustar\n",
        "    # Guardar la predicción\n",
        "\n",
        "sns.lineplot(x=k_values, y=scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSgPG286QcwO"
      },
      "source": [
        "Con el valor que hayáis obtenido de la gráfica, podéis obtener una buena aproximación de Kmeans y con ello podemos pasar a explorar cómo de bien han separado la información los distintos clusters. Para ello, se va a hacer un ``catplot``, seaborn os lo hará solito. Con esto lo que se pretende ver es la distribución de la varaible a predecir en función del cluster que haya determinado Kmeans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa7XfETyQcwO",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Aprender Kmeans con el valor de K obtenido.\n",
        "\n",
        "kmeans = # Definir y entrenar Kmeans.\n",
        "\n",
        "# Preparar el catplot.\n",
        "\n",
        "\n",
        "# Pintar.\n",
        "ax = sns.catplot(col=, x=, data=, kind='count',col_wrap=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzMUKFwzQcwO"
      },
      "source": [
        "Vamos a ver qué tal queda esto pintado. Para ello, repetimos el scatterplot de antes pero usando como color el cluster asignado por kmeans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjhjuexcQcwO",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Entrenar PCA para representar.\n",
        "\n",
        "# Usar un color por cada cluster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0q-ZDhQQcwO"
      },
      "source": [
        "¿Es bastante parecido no? No es tan bueno como el Random Forest, pero ha conseguido identificar bastante bien los distintos puntos del dataset sin utilizar las etiquetas. De hecho, el diagrama de factor que hemos visto antes muestra que solo un par de clusters son imprecisos. Si no hubieramos tenido etiquetas esta aproximacion nos hubiera ayudado mucho a clasificar los distintos tipos de hongos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
